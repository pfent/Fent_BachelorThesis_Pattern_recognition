\chapter{Evaluation}\label{chapter:evaluation}
In this chapter, we evaluate the currently implemented prototypes in regards to efficiently, accurately, and practically extract behavioral patterns that identify individual users. First of all, we are measuring the runtime and performance of our implementation to verify, that the used approaches are practical. With a practical solution, we subsequently compare different parameters for the used algorithms to measure how accurately users can be identified with the app prototypes.

\section{Test setup}
For testing and comparing different approaches, we implemented a test setup that easily allows replication and validation of tests. In order to provide such a test setup, we are using test data generated by 5 students of the Android Practical course. The raw motion data of the users has been recorded to a database, which can be found on the attached CD, called {\ttfamily Sensor\-Measurements\_\-tablet\_\-usertest.db}, respectively {\ttfamily Sensor\-Measurements\_\-wear\_\-usertest.db}. In this database we recorded 5 users with 10 measurements each.

The Android measurements have been conducted on a Nexus 10 Android tablet. In this test, each user first entered his name, then ten times the sequence of characters: ``helloworld''. The recorded data then was saved to the  {\ttfamily Sensor\-Measurements\_\-tablet\_\-usertest.db} database. For the Android Wear app, we used a Sony Smartwatch 3 to record the data. In our test setup, we did not individualize the measurements promtly, but simply edited the \lstinline$userID$ in the database according to the users afterwards. In the Android Wear test, the users each entered the character sequence: ``correcthorsebatterystaple'' ten times.

To replicate these tests, the databases can be copied to the apps databases folder as \lstinline$SensorMeasurements.db$. Executing the \lstinline$EvaluationActivity$ will process the data and write any intermediate results to a \lstinline$CacheFeatures.db$ database, located in the cache folder of the app. The Android app also logs detected users to Android's logging system, which can be viewed with \lstinline$adb logcat$.

The code test setup is to first run the preprocess and feature extraction steps on all measurements. With these features we can train our classification algorithm. For this we use all but one data set for each user. With this trained classifier, we can measure our detection rate for the remaining data set.

\section{Runtime efficiency}\label{section:runtime}
Since the current setup is mainly used for debugging, we are logging the results and itermediates to an additional database. This results in a relatively poor performance of \num{4.3} measurements per second on a OnePlus One smartphone. However this test is not limited by the processor, but by the database and the storage write speeds. In an optimized environment, where we turn of writing to the database and thus keep all data in memory only, we get much better results. Our tests showed, that with optimizations, we can reach $\sim$\num{21} measurements per second.

We now reached an almost \num{100}$\%$ CPU utilization of one core. A possible additional optimization would be parallelization. An evaluation showed, that the preprocessing and feature extraction can completely be done independently from one another. This could potentially speed up the learning phase by a factor of \SI{4}{x}.

On Smartwatches, significantly less processing power is available, thus processing the measurements should take proportionally longer. Expectedly, measurements on a Sony Smartwatch 3 showed a throughput of \num{1.8} measurements per second with logging enabled. This is roughly proportional to the clock speed of the smartwatch's processor. Potential optimizations are the same as on a smartphone, \ie disable database writing and parallelization, with potential speedups of $\sim$\SI{5}{x}. 

As a note, transferring the data from the watch to the smartphone for processing would not speed up the computation time, since measured Bluetooth transfer rates of $\sim$\SI{15}{MBps} are slower than the processing throughput on the watch itself.

\section{Optimal parameters for accuracy}\label{section:parameters}
In our test setup, there are in total four tunable parameters, that influence the classification results. Starting with feature extraction, we can set the window size and the stringency for the peak detection algorithm. The window size is essentially the time between two detected peak and thus limits the maximum detected features. On the other hand the stringency defines, how ``big'' a peak needs to be in order to be recognized.

For classification, we can influence the window size of the \gls{dtw} algorithm, \ie how many false detections we can tolerate. We can also set the $k$ parameter in the \gls{knn} algorithm, influencing cluster sizes.

We found, that the window size of \num{67} we calculated in Equation~\ref{eq:windowsize}, is in fact outperforming bigger or smaller window sizes. With the stringency however, it was not exactly clear, what the optimal value looks like. Palshikar \cite{palshikar2009simple} recommends, that the stringency $h$ should typically be $1 \leq h \leq 3$. Comparing the overall results of the stringency, we found that a value of 2 resulted in the best extracted features. The \gls{dtw} window size did not affect the results positively with bigger window sizes, so we used a relatively small value of 3. With the nearest neighbor algorithm, we got the best results with a $k = 7$.

With these optimal parameters, we reached an peak detection rate of 80\%, \ie 4 out of 5 users correctly classified.